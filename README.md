# â›ï¸ Minecraft agent player for [ã‚¢ã‚¤ãƒª (AIRI)](https://airi.moeru.ai)

> [!NOTE]
>
> This project is part of the [Project ã‚¢ã‚¤ãƒª (AIRI)](https://github.com/moeru-ai/airi), we aim to build a LLM-driven VTuber like [Neuro-sama](https://www.youtube.com/@Neurosama) (subscribe if you didn't!) if you are interested in, please do give it a try on [live demo](https://airi.moeru.ai).

An intelligent Minecraft bot powered by LLM. AIRI can understand natural language commands, interact with the world, and assist players in various tasks.

## ğŸ¥ Preview

![demo](./docs/preview.avif)

## âœ¨ Features

- ğŸ—£ï¸ Natural language understanding
- ğŸƒâ€â™‚ï¸ Advanced pathfinding and navigation
- ğŸ› ï¸ Block breaking and placing
- ğŸ¯ Combat and PvP capabilities
- ğŸ”„ Auto-reconnect on disconnection
- ğŸ“¦ Inventory management
- ğŸ¤ Player following and interaction
- ğŸŒ World exploration and mapping

## ğŸš€ Getting Started

### ğŸ“‹ Prerequisites

- ğŸ“¦ Node.js 23+
- ğŸ”§ pnpm
- ğŸ® A Minecraft server (1.20+)

### ğŸ”¨ Installation

1. Clone the repository:

```bash
git clone https://github.com/moeru-ai/airi.git
cd services/minecraft
```

2. Install dependencies:

```bash
pnpm install
```

3. Create a `.env.local` file with your configuration:

> [!NOTE]
> For all online accounts, un-comment the following line to toggle Microsoft authentication.
> Link for authentication will popup when the bot starts.
>
> After signed in, according to [how Minecraft protocol was implemented](https://github.com/PrismarineJS/node-minecraft-protocol/blob/bf89f7e86526c54d8c43f555d8f6dfa4948fd2d9/src/client/microsoftAuth.js#L7-L16)
> and also, [authentication flow implemented here](https://github.com/PrismarineJS/prismarine-auth/blob/1aef6e1387d94fca839f2811d17ac6659ae556b4/src/MicrosoftAuthFlow.js#L59-L69),
> the token will be cached with [the cache IDs specified here](https://github.com/PrismarineJS/prismarine-auth/blob/1aef6e1387d94fca839f2811d17ac6659ae556b4/src/MicrosoftAuthFlow.js#L88-L93)
> in split files:
>
> - `${hash}_live-cache.json`
> - `${hash}_mca-cache.json`
> - `${hash}_xbl-cache.json`
>
> inside of the directory provided by [`minecraft-folder-path`](https://github.com/simonmeusel/minecraft-folder-path)
>
> Linux: `~/.minecraft/nmp-cache/`
> macOS: `~/Library/Application Support/minecraft/nmp-cache/`
> Windows: `%appdata%/.minecraft/nmp-cache/`
>
> where `${hash}` is the `sha1` hash of the username you signing in with (as Minecraft username).

```env
OPENAI_API_KEY=your_openai_api_key
OPENAI_API_BASEURL=your_openai_api_baseurl

BOT_USERNAME=your_bot_username
BOT_HOSTNAME=localhost
BOT_PORT=25565
BOT_AUTH='microsoft' # comment if you use offline mode
BOT_VERSION=1.20
```

1. Start the bot:

```bash
pnpm dev
```

## ğŸ® Usage

Once the bot is connected, you can interact with it using chat commands in Minecraft. All commands start with `#`.

### Basic Commands

- `#help` - Show available commands
- `#follow` - Make the bot follow you
- `#stop` - Stop the current action
- `#come` - Make the bot come to your location

### Natural Language Commands

You can also give the bot natural language commands, and it will try to understand and execute them. For example:

- "Build a house"
- "Find some diamonds"
- "Help me fight these zombies"
- "Collect wood from nearby trees"

## ğŸ§  Cognitive Architecture

AIRI's Minecraft agent is built on a **three-layered cognitive architecture** inspired by cognitive science, enabling both reactive and deliberate behaviors. This design allows the bot to respond instantly to urgent situations while maintaining the ability to plan and execute complex tasks.

### Architecture Overview

```mermaid
graph TB
    subgraph "Layer A: Perception"
        Events[Raw Events]
        EM[Event Manager]
        Events --> EM
    end

    subgraph "Layer B: Reflex (Subconscious)"
        RM[Reflex Manager]
        FSM[State Machine]
        RM --> FSM
    end

    subgraph "Layer C: Conscious"
        ORC[Orchestrator]
        Planning[Planning Agent]
        Action[Action Agent]
        Chat[Chat Agent]
        ORC --> Planning
        ORC --> Action
        ORC --> Chat
    end

    EM -->|High Priority| RM
    EM -->|All Events| ORC
    RM -.->|Inhibition Signal| ORC

    style EM fill:#e1f5ff
    style RM fill:#fff4e1
    style ORC fill:#ffe1f5
```

### Layer A: Perception

**Location**: `src/cognitive/perception/`

The perception layer acts as the sensory input hub, receiving and preprocessing all events from the Minecraft world and external sources.

**Components**:
- **Event Manager** (`event-manager.ts`): Centralized event distribution system
  - Emits standardized `BotEvent` objects
  - Supports event prioritization (TODO: salience detection)
  - Manages temporal context (TODO: short-term event memory)

**Event Types**:
- `user_intent`: Player chat messages, voice commands
- `world_update`: Block changes, entity movements, damage events
- `system_alert`: Internal system notifications

**Event Flow**:
```typescript
// Example: Chat message â†’ Event
{
  type: 'user_intent',
  payload: { content: 'build a house' },
  source: { type: 'minecraft', id: 'player123' },
  timestamp: 1234567890,
  priority: 0,  // Default priority
  handled: false // Not yet processed
}
```

### Layer B: Reflex

**Location**: `src/cognitive/reflex/`

The reflex layer handles immediate, instinctive reactions without LLM overhead. It operates on a finite state machine (FSM) pattern for predictable, fast responses.

**Components**:
- **Reflex Manager** (`reflex-manager.ts`): Coordinates all reflex behaviors
  - Subscribes to high-priority events
  - Executes instant responses
  - Sets inhibition signals to prevent unnecessary LLM calls

**Current Reflexes**:
- âœ… **Greeting Reflex**: Instantly responds to "hi" or "hello"
- ğŸš§ **Dodge Reflex** (TODO): Avoid incoming projectiles
- ğŸš§ **Survival Reflex** (TODO): Auto-eat when hungry, flee from danger

**Inhibition Mechanism**:
When a reflex handles an event, it sets `event.handled = true`, preventing the expensive Conscious layer from processing the same event.

```typescript
// Example: Greeting reflex
if (content === 'hi') {
  bot.chat('Hi there! (Reflex)')
  event.handled = true // Inhibit Conscious processing
}
```

### Layer C: Conscious

**Location**: `src/cognitive/conscious/`

The conscious layer handles complex reasoning, planning, and decision-making using LLM-powered agents.

**Components**:
- **Orchestrator** (`orchestrator.ts`): Main coordinator for deliberate actions
  - Checks inhibition signals from Reflex layer
  - Manages processing state (prevents concurrent operations)
  - Coordinates Planning â†’ Execution â†’ Response flow

- **Planning Agent**: Creates multi-step plans to achieve goals
- **Action Agent**: Executes atomic actions (move, mine, build)
- **Chat Agent**: Generates natural language responses

**Processing Pipeline**:
```
1. Check Inhibition â†’ 2. Update Memory â†’ 3. Create Plan â†’
4. Execute Actions â†’ 5. Generate Response â†’ 6. Reply
```

**State Management**:
- Uses `isProcessing` lock to prevent race conditions
- Future: Queue system for handling concurrent intents

### ğŸ”„ Event Flow Example

**Scenario 1: Simple Greeting (Reflex)**
```
Player: "hi"
  â†“
[Perception] EventManager emits user_intent
  â†“
[Reflex] ReflexManager detects greeting â†’ Replies instantly
  â†“
[Conscious] Orchestrator sees handled=true â†’ Skips processing
```

**Scenario 2: Complex Command (Conscious)**
```
Player: "build a house"
  â†“
[Perception] EventManager emits user_intent
  â†“
[Reflex] ReflexManager ignores (not a reflex trigger)
  â†“
[Conscious] Orchestrator processes:
  - PlanningAgent creates building plan
  - ActionAgent executes steps (gather, place blocks)
  - ChatAgent generates response
  â†“
Bot: "I've built a small house for you!"
```

### ğŸ“ Project Structure

```
src/
â”œâ”€â”€ cognitive/              # ğŸ§  Three-layer cognitive system
â”‚   â”œâ”€â”€ perception/        # Layer A: Event processing
â”‚   â”‚   â””â”€â”€ event-manager.ts
â”‚   â”œâ”€â”€ reflex/            # Layer B: Instant reactions
â”‚   â”‚   â””â”€â”€ reflex-manager.ts
â”‚   â”œâ”€â”€ conscious/         # Layer C: LLM-powered reasoning
â”‚   â”‚   â”œâ”€â”€ orchestrator.ts
â”‚   â”‚   â”œâ”€â”€ completion.ts
â”‚   â”‚   â”œâ”€â”€ prompt.ts
â”‚   â”‚   â””â”€â”€ handler.ts
â”‚   â”œâ”€â”€ container.ts       # Dependency injection
â”‚   â”œâ”€â”€ index.ts           # Cognitive system entry
â”‚   â””â”€â”€ types.ts           # Shared type definitions
â”œâ”€â”€ agents/                # Specialized AI agents
â”‚   â”œâ”€â”€ action/           # Action execution agent
â”‚   â”œâ”€â”€ planning/         # Goal planning agent
â”‚   â””â”€â”€ chat/             # Conversation agent
â”œâ”€â”€ libs/
â”‚   â””â”€â”€ mineflayer/       # Mineflayer bot wrapper
â”œâ”€â”€ skills/               # Atomic bot capabilities
â”œâ”€â”€ composables/          # Reusable functions
â””â”€â”€ utils/                # Helper utilities
```

### ğŸ¯ Design Principles

1. **Separation of Concerns**: Each layer has a distinct responsibility
2. **Event-Driven**: Loose coupling via centralized event system
3. **Inhibition Control**: Reflexes prevent unnecessary LLM calls
4. **Extensibility**: Easy to add new reflexes or conscious behaviors
5. **Cognitive Realism**: Mimics human-like perception â†’ reaction â†’ deliberation

### ğŸš§ Future Enhancements

- **Perception Layer**:
  - â±ï¸ Temporal context window (remember recent events)
  - ğŸ¯ Salience detection (filter noise, prioritize important events)

- **Reflex Layer**:
  - ğŸƒ Dodge hostile mobs
  - ğŸ– Auto-eat when health/hunger is low
  - ğŸ›¡ï¸ Emergency combat responses

- **Conscious Layer**:
  - ğŸ’­ Emotional state management
  - ğŸ§  Long-term memory integration
  - ğŸ­ Personality-driven responses

## ğŸ› ï¸ Development

### Commands

- `pnpm dev` - Start the bot in development mode
- `pnpm lint` - Run ESLint
- `pnpm typecheck` - Run TypeScript type checking
- `pnpm test` - Run tests

## ğŸ™ Acknowledgements

- https://github.com/kolbytn/mindcraft

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
